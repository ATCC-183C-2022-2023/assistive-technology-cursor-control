{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import cv2\n",
    "import sklearn\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GESTURE_TYPES = 11\n",
    "LABEL_DICT = {k:i for i,k in enumerate([21, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33])}\n",
    "CONNECTION_LABELS = [\n",
    "    (0, 1), (1, 2), (2, 3), (3, 4),\n",
    "    (5, 6), (6, 7), (7, 8),\n",
    "    (9, 10), (10, 11), (11, 12),\n",
    "    (13, 14), (14, 15), (15, 16),\n",
    "    (17, 18), (18, 19), (19, 20),\n",
    "    (0, 5), (5, 9), (9, 13), (13, 17), (0, 17)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23346, 5, 21, 3) (23346,)\n"
     ]
    }
   ],
   "source": [
    "def load_keypoint_sequences(data_path='gesture_recognition/Fall 2020/data', sequence_length=5):\n",
    "    keypoints = []\n",
    "    labels = []\n",
    "    for subjectName in os.listdir(data_path):\n",
    "        if not (subjectName.startswith(\"Subject\") or subjectName.startswith(\"subject\")): continue\n",
    "        # subjectNum = int(re.findall(r'(\\d+)', subjectName)[0])\n",
    "        for sceneName in os.listdir(os.path.join(data_path, subjectName)):\n",
    "            if not (sceneName.startswith(\"Scene\") or subjectName.startswith(\"scene\")): continue\n",
    "            for groupEntry in os.scandir(os.path.join(data_path, subjectName, sceneName)):\n",
    "                with open(groupEntry, 'r') as f:\n",
    "                    groupData = json.load(f)\n",
    "                    for gesture in groupData:\n",
    "                        # print(gesture['label'], gesture['keypoints'])\n",
    "                        for i in range(len(gesture['keypoints'])):\n",
    "                            if not gesture['keypoints'][i]:\n",
    "                                gesture['keypoints'][i] = [[np.nan, np.nan, np.nan] for _ in range(21)]\n",
    "                        for start_frame in range(len(gesture['keypoints']) - sequence_length + 1):\n",
    "                            keypoints.append(gesture['keypoints'][start_frame: start_frame + sequence_length])\n",
    "                            labels.append(LABEL_DICT[gesture['label']])\n",
    "    keypoints = np.array(keypoints)\n",
    "    labels = np.array(labels)\n",
    "    return keypoints, labels\n",
    "keypoints, labels = load_keypoint_sequences()\n",
    "print(keypoints.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_connection_angles_from_sequences(keypoints, keypoints_num=21, keypoints_dimensions=3):\n",
    "    connections = []\n",
    "    for connection in CONNECTION_LABELS:\n",
    "        connections.append(keypoints[..., connection[1], :] - keypoints[..., connection[0], :])\n",
    "    connections = np.stack(connections, axis = -2)\n",
    "    tensor1 = connections[..., np.newaxis].repeat(keypoints_num, -1).transpose(0,1,2,4,3)\n",
    "    tensor2 = connections[..., np.newaxis].repeat(keypoints_num, -1).transpose(0,1,4,2,3)\n",
    "    angles = (tensor1*tensor2).sum(axis=-1)/np.linalg.norm(tensor1,axis=-1)/np.linalg.norm(tensor2,axis=-1)\n",
    "    angles = angles.transpose(2,3,0,1)[np.triu_indices(21, k = 1)].transpose(1,2,0)\n",
    "    return np.arccos(angles)\n",
    "angles = generate_connection_angles_from_sequences(keypoints)\n",
    "print(angles.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_joint_distances_from_sequences(keypoints, keypoints_num=21, keypoints_dimensions=3):\n",
    "    connections = []\n",
    "    for connection in CONNECTION_LABELS:\n",
    "        connections.append(keypoints[..., connection[1], :] - keypoints[..., connection[0], :])\n",
    "    connections = np.stack(connections, axis = -2)\n",
    "    tensor1 = connections[..., np.newaxis].repeat(keypoints_num, -1).transpose(0,1,2,4,3)\n",
    "    tensor2 = connections[..., np.newaxis].repeat(keypoints_num, -1).transpose(0,1,4,2,3)\n",
    "    distances = np.linalg.norm(tensor1-tensor2,axis=-1).transpose(2,3,0,1)[np.triu_indices(21, k = 1)].transpose(1,2,0)\n",
    "    return distances\n",
    "distances = generate_joint_distances_from_sequences(keypoints)\n",
    "print(distances.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_keypoint_sequences(keypoints):\n",
    "    for sequence in keypoints:\n",
    "        for points in sequence:\n",
    "            img = np.zeros((480, 640, 3))\n",
    "            for point in points:\n",
    "                x, y, z = point\n",
    "                if np.isnan(x):\n",
    "                    continue\n",
    "                cv2.circle(img, (int(x), int(y)), 4, (255, 0, 0), 2)\n",
    "            for connection in CONNECTION_LABELS:\n",
    "                if np.isnan(points[connection[0]][0]):\n",
    "                    continue\n",
    "                x0, y0, z0 = points[connection[0]]\n",
    "                x1, y1, z1 = points[connection[1]]\n",
    "                cv2.line(img, (int(x0), int(y0)), (int(x1), int(y1)), (0, 255, 0), 2)\n",
    "            cv2.imshow(\"Key Points\", img)\n",
    "            key = cv2.waitKey(1)\n",
    "            if key == 27:\n",
    "                cv2.destroyAllWindows()\n",
    "                cv2.waitKey(1) # cannot close window on macOS without this line\n",
    "                return\n",
    "# visualize_keypoint_sequences(keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18250, 5, 483)\n"
     ]
    }
   ],
   "source": [
    "def process_sequence_features(keypoints, angles, distances):\n",
    "    data_length = keypoints.shape[0]\n",
    "    sequence_length = keypoints.shape[1]\n",
    "    keypoints = keypoints.reshape(data_length*sequence_length, -1)\n",
    "    angles = angles.reshape(data_length*sequence_length, -1)\n",
    "    distances = distances.reshape(data_length*sequence_length, -1)\n",
    "    features = np.concatenate((keypoints, angles, distances), -1)\n",
    "    df = pd.DataFrame(features)\n",
    "    df = (df-df.mean())/df.std()\n",
    "    df = df.fillna(0)\n",
    "    features = df.to_numpy().reshape(data_length, sequence_length, -1)\n",
    "    return features\n",
    "X = process_sequence_features(keypoints, angles, distances)\n",
    "#normalizer = tf.keras.layers.experimental.preprocessing.Normalization()\n",
    "#normalizer.adapt(X)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(processed_keypoints, labels, test_size=0.2, random_state=0)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "457/457 [==============================] - 3s 5ms/step - loss: 0.9810 - accuracy: 0.6862 - val_loss: 0.8193 - val_accuracy: 0.8044\n",
      "Epoch 2/20\n",
      "457/457 [==============================] - 2s 4ms/step - loss: 0.4230 - accuracy: 0.8736 - val_loss: 0.7648 - val_accuracy: 0.8115\n",
      "Epoch 3/20\n",
      "457/457 [==============================] - 2s 4ms/step - loss: 0.3609 - accuracy: 0.8897 - val_loss: 0.9026 - val_accuracy: 0.8110\n",
      "Epoch 4/20\n",
      "457/457 [==============================] - 2s 4ms/step - loss: 0.3389 - accuracy: 0.8955 - val_loss: 0.9116 - val_accuracy: 0.8252\n",
      "Epoch 5/20\n",
      "457/457 [==============================] - 2s 4ms/step - loss: 0.3046 - accuracy: 0.9104 - val_loss: 0.9243 - val_accuracy: 0.8293\n",
      "Epoch 6/20\n",
      "457/457 [==============================] - 2s 4ms/step - loss: 0.2909 - accuracy: 0.9149 - val_loss: 1.0533 - val_accuracy: 0.8156\n",
      "Epoch 7/20\n",
      "457/457 [==============================] - 2s 4ms/step - loss: 0.2750 - accuracy: 0.9170 - val_loss: 1.0856 - val_accuracy: 0.8266\n",
      "Epoch 8/20\n",
      "457/457 [==============================] - 2s 4ms/step - loss: 0.2761 - accuracy: 0.9170 - val_loss: 1.1597 - val_accuracy: 0.8192\n",
      "Epoch 9/20\n",
      "457/457 [==============================] - 2s 4ms/step - loss: 0.2519 - accuracy: 0.9237 - val_loss: 1.1942 - val_accuracy: 0.8266\n",
      "Epoch 10/20\n",
      "457/457 [==============================] - 2s 5ms/step - loss: 0.2681 - accuracy: 0.9190 - val_loss: 1.1749 - val_accuracy: 0.8164\n",
      "Epoch 11/20\n",
      "457/457 [==============================] - 2s 5ms/step - loss: 0.2558 - accuracy: 0.9226 - val_loss: 1.2350 - val_accuracy: 0.8208\n",
      "Epoch 12/20\n",
      "457/457 [==============================] - 2s 5ms/step - loss: 0.2443 - accuracy: 0.9237 - val_loss: 1.2224 - val_accuracy: 0.8140\n",
      "Epoch 13/20\n",
      "457/457 [==============================] - 2s 5ms/step - loss: 0.2402 - accuracy: 0.9310 - val_loss: 1.1242 - val_accuracy: 0.8115\n",
      "Epoch 14/20\n",
      "457/457 [==============================] - 2s 5ms/step - loss: 0.2408 - accuracy: 0.9281 - val_loss: 1.1980 - val_accuracy: 0.8282\n",
      "Epoch 15/20\n",
      "457/457 [==============================] - 3s 6ms/step - loss: 0.2351 - accuracy: 0.9294 - val_loss: 1.2927 - val_accuracy: 0.8184\n",
      "Epoch 16/20\n",
      "457/457 [==============================] - 3s 6ms/step - loss: 0.2194 - accuracy: 0.9344 - val_loss: 1.2182 - val_accuracy: 0.8249\n",
      "Epoch 17/20\n",
      "457/457 [==============================] - 3s 6ms/step - loss: 0.2206 - accuracy: 0.9349 - val_loss: 1.3782 - val_accuracy: 0.8189\n",
      "Epoch 18/20\n",
      "457/457 [==============================] - 3s 6ms/step - loss: 0.2403 - accuracy: 0.9282 - val_loss: 1.2698 - val_accuracy: 0.8299\n",
      "Epoch 19/20\n",
      "457/457 [==============================] - 3s 6ms/step - loss: 0.2229 - accuracy: 0.9340 - val_loss: 1.2373 - val_accuracy: 0.8304\n",
      "Epoch 20/20\n",
      "457/457 [==============================] - 3s 6ms/step - loss: 0.2189 - accuracy: 0.9335 - val_loss: 1.2892 - val_accuracy: 0.8288\n"
     ]
    }
   ],
   "source": [
    "model_lstm = tf.keras.Sequential([layers.Masking() ,layers.LSTM(GESTURE_TYPES, activation=None), layers.Activation('softmax')])\n",
    "model_lstm.compile(loss=tf.losses.SparseCategoricalCrossentropy(), optimizer=tf.optimizers.Adam(), metrics=['accuracy'])\n",
    "history = model_lstm.fit(X, labels, epochs=20, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru = tf.keras.Sequential([layers.Masking() ,layers.GRU(GESTURE_TYPES, activation=None), layers.Activation('softmax')])\n",
    "model_gru.compile(loss=tf.losses.SparseCategoricalCrossentropy(), optimizer=tf.optimizers.Adam(), metrics=['accuracy'])\n",
    "history = model_gru.fit(X, labels, epochs=20, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU cells work just as well as LSTM cells, and is cheaper in theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18676 samples, validate on 4670 samples\n",
      "Epoch 1/20\n",
      "18676/18676 [==============================] - 18s 946us/sample - loss: 0.6432 - accuracy: 0.8113 - val_loss: 0.6360 - val_accuracy: 0.8109\n",
      "Epoch 2/20\n",
      "18676/18676 [==============================] - 12s 664us/sample - loss: 0.3715 - accuracy: 0.8841 - val_loss: 0.6390 - val_accuracy: 0.8193\n",
      "Epoch 3/20\n",
      "18676/18676 [==============================] - 13s 682us/sample - loss: 0.3176 - accuracy: 0.8992 - val_loss: 0.6400 - val_accuracy: 0.8188\n",
      "Epoch 4/20\n",
      "18676/18676 [==============================] - 13s 691us/sample - loss: 0.2875 - accuracy: 0.9074 - val_loss: 0.6890 - val_accuracy: 0.8135\n",
      "Epoch 5/20\n",
      "18676/18676 [==============================] - 13s 699us/sample - loss: 0.2707 - accuracy: 0.9118 - val_loss: 0.6987 - val_accuracy: 0.8218\n",
      "Epoch 6/20\n",
      "18676/18676 [==============================] - 14s 737us/sample - loss: 0.2518 - accuracy: 0.9185 - val_loss: 0.6891 - val_accuracy: 0.8263\n",
      "Epoch 7/20\n",
      "18676/18676 [==============================] - 20s 1ms/sample - loss: 0.2388 - accuracy: 0.9223 - val_loss: 0.7309 - val_accuracy: 0.8143\n",
      "Epoch 8/20\n",
      "18676/18676 [==============================] - 20s 1ms/sample - loss: 0.2317 - accuracy: 0.9221 - val_loss: 0.7530 - val_accuracy: 0.8244\n",
      "Epoch 9/20\n",
      "18676/18676 [==============================] - 20s 1ms/sample - loss: 0.2559 - accuracy: 0.9195 - val_loss: 0.8123 - val_accuracy: 0.8186\n",
      "Epoch 10/20\n",
      "18676/18676 [==============================] - 20s 1ms/sample - loss: 0.2333 - accuracy: 0.9254 - val_loss: 0.7412 - val_accuracy: 0.8304\n",
      "Epoch 11/20\n",
      "18676/18676 [==============================] - 21s 1ms/sample - loss: 0.1943 - accuracy: 0.9357 - val_loss: 0.8091 - val_accuracy: 0.8210\n",
      "Epoch 12/20\n",
      "18676/18676 [==============================] - 21s 1ms/sample - loss: 0.1863 - accuracy: 0.9390 - val_loss: 0.8298 - val_accuracy: 0.8270\n",
      "Epoch 13/20\n",
      "18676/18676 [==============================] - 21s 1ms/sample - loss: 0.1975 - accuracy: 0.9352 - val_loss: 0.8589 - val_accuracy: 0.8197\n",
      "Epoch 14/20\n",
      "18676/18676 [==============================] - 21s 1ms/sample - loss: 0.1831 - accuracy: 0.9398 - val_loss: 0.8075 - val_accuracy: 0.8201\n",
      "Epoch 15/20\n",
      "18676/18676 [==============================] - 21s 1ms/sample - loss: 0.1760 - accuracy: 0.9423 - val_loss: 0.9116 - val_accuracy: 0.8111\n",
      "Epoch 16/20\n",
      "18676/18676 [==============================] - 16s 865us/sample - loss: 0.1725 - accuracy: 0.9433 - val_loss: 0.8570 - val_accuracy: 0.8201\n",
      "Epoch 17/20\n",
      "18676/18676 [==============================] - 20s 1ms/sample - loss: 0.1788 - accuracy: 0.9420 - val_loss: 0.9436 - val_accuracy: 0.8223\n",
      "Epoch 18/20\n",
      "18676/18676 [==============================] - 20s 1ms/sample - loss: 0.1691 - accuracy: 0.9446 - val_loss: 0.8958 - val_accuracy: 0.8225\n",
      "Epoch 19/20\n",
      "18676/18676 [==============================] - 20s 1ms/sample - loss: 0.1632 - accuracy: 0.9459 - val_loss: 1.0736 - val_accuracy: 0.8208\n",
      "Epoch 20/20\n",
      "18676/18676 [==============================] - 21s 1ms/sample - loss: 0.1745 - accuracy: 0.9421 - val_loss: 1.0885 - val_accuracy: 0.8133\n"
     ]
    }
   ],
   "source": [
    "model_bilstm = tf.keras.Sequential([layers.Masking(), tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(GESTURE_TYPES, activation=None)), tf.keras.layers.Activation('softmax')])\n",
    "model_bilstm.compile(loss=tf.losses.SparseCategoricalCrossentropy(), optimizer=tf.optimizers.Adam(), metrics=['accuracy'])\n",
    "history = model_bilstm.fit(X, labels, epochs=20, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making LSTM bidirectional does not improve the accuracy by a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18676 samples, validate on 4670 samples\n",
      "Epoch 1/20\n",
      "18676/18676 [==============================] - 12s 623us/sample - loss: 0.9693 - accuracy: 0.7050 - val_loss: 0.8205 - val_accuracy: 0.7458\n",
      "Epoch 2/20\n",
      "18676/18676 [==============================] - 7s 381us/sample - loss: 0.5730 - accuracy: 0.8219 - val_loss: 0.7488 - val_accuracy: 0.7747\n",
      "Epoch 3/20\n",
      "18676/18676 [==============================] - 7s 349us/sample - loss: 0.4934 - accuracy: 0.8461 - val_loss: 0.7004 - val_accuracy: 0.7895\n",
      "Epoch 4/20\n",
      "18676/18676 [==============================] - 6s 332us/sample - loss: 0.4533 - accuracy: 0.8579 - val_loss: 0.7124 - val_accuracy: 0.7979\n",
      "Epoch 5/20\n",
      "18676/18676 [==============================] - 7s 360us/sample - loss: 0.4359 - accuracy: 0.8646 - val_loss: 0.7233 - val_accuracy: 0.7989\n",
      "Epoch 6/20\n",
      "18676/18676 [==============================] - 7s 350us/sample - loss: 0.4157 - accuracy: 0.8695 - val_loss: 0.6869 - val_accuracy: 0.8032\n",
      "Epoch 7/20\n",
      "18676/18676 [==============================] - 7s 363us/sample - loss: 0.4016 - accuracy: 0.8743 - val_loss: 0.6955 - val_accuracy: 0.8054\n",
      "Epoch 8/20\n",
      "18676/18676 [==============================] - 7s 356us/sample - loss: 0.3886 - accuracy: 0.8777 - val_loss: 0.7214 - val_accuracy: 0.8060\n",
      "Epoch 9/20\n",
      "18676/18676 [==============================] - 7s 369us/sample - loss: 0.3873 - accuracy: 0.8779 - val_loss: 0.6874 - val_accuracy: 0.8103\n",
      "Epoch 10/20\n",
      "18676/18676 [==============================] - 6s 348us/sample - loss: 0.3698 - accuracy: 0.8833 - val_loss: 0.7941 - val_accuracy: 0.8030\n",
      "Epoch 11/20\n",
      "18676/18676 [==============================] - 7s 365us/sample - loss: 0.3661 - accuracy: 0.8838 - val_loss: 0.7394 - val_accuracy: 0.8146\n",
      "Epoch 12/20\n",
      "18676/18676 [==============================] - 7s 371us/sample - loss: 0.3559 - accuracy: 0.8858 - val_loss: 0.7138 - val_accuracy: 0.8081\n",
      "Epoch 13/20\n",
      "18676/18676 [==============================] - 7s 363us/sample - loss: 0.3476 - accuracy: 0.8899 - val_loss: 0.6824 - val_accuracy: 0.8101\n",
      "Epoch 14/20\n",
      "18676/18676 [==============================] - 6s 339us/sample - loss: 0.3472 - accuracy: 0.8889 - val_loss: 0.7193 - val_accuracy: 0.7974\n",
      "Epoch 15/20\n",
      "18676/18676 [==============================] - 7s 356us/sample - loss: 0.3404 - accuracy: 0.8894 - val_loss: 0.6783 - val_accuracy: 0.8156\n",
      "Epoch 16/20\n",
      "18676/18676 [==============================] - 7s 370us/sample - loss: 0.3311 - accuracy: 0.8936 - val_loss: 0.7142 - val_accuracy: 0.8143\n",
      "Epoch 17/20\n",
      "18676/18676 [==============================] - 7s 369us/sample - loss: 0.3233 - accuracy: 0.8966 - val_loss: 0.6599 - val_accuracy: 0.8186\n",
      "Epoch 18/20\n",
      "18676/18676 [==============================] - 7s 393us/sample - loss: 0.3244 - accuracy: 0.8956 - val_loss: 0.7102 - val_accuracy: 0.8081\n",
      "Epoch 19/20\n",
      "18676/18676 [==============================] - 7s 368us/sample - loss: 0.3247 - accuracy: 0.8963 - val_loss: 0.6951 - val_accuracy: 0.8150\n",
      "Epoch 20/20\n",
      "18676/18676 [==============================] - 7s 363us/sample - loss: 0.3174 - accuracy: 0.8992 - val_loss: 0.7412 - val_accuracy: 0.8135\n"
     ]
    }
   ],
   "source": [
    "model_rnn = tf.keras.Sequential([layers.Masking() ,tf.keras.layers.SimpleRNN(GESTURE_TYPES, activation=None), tf.keras.layers.Activation('softmax')])\n",
    "model_rnn.compile(loss=tf.losses.SparseCategoricalCrossentropy(), optimizer=tf.optimizers.Adam(), metrics=['accuracy'])\n",
    "history = model_rnn.fit(X, labels, epochs=20, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple RNN gives a slightly worse performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18676 samples, validate on 4670 samples\n",
      "Epoch 1/20\n",
      "18676/18676 [==============================] - 25s 1ms/sample - loss: 0.4140 - accuracy: 0.8728 - val_loss: 0.5526 - val_accuracy: 0.8223\n",
      "Epoch 2/20\n",
      "18676/18676 [==============================] - 20s 1ms/sample - loss: 0.2272 - accuracy: 0.9288 - val_loss: 0.5408 - val_accuracy: 0.8332\n",
      "Epoch 3/20\n",
      "18676/18676 [==============================] - 21s 1ms/sample - loss: 0.1595 - accuracy: 0.9516 - val_loss: 0.5519 - val_accuracy: 0.8379\n",
      "Epoch 4/20\n",
      "18676/18676 [==============================] - 20s 1ms/sample - loss: 0.1228 - accuracy: 0.9641 - val_loss: 0.5707 - val_accuracy: 0.8281\n",
      "Epoch 5/20\n",
      "18676/18676 [==============================] - 20s 1ms/sample - loss: 0.0874 - accuracy: 0.9768 - val_loss: 0.5883 - val_accuracy: 0.8362\n",
      "Epoch 6/20\n",
      "18676/18676 [==============================] - 21s 1ms/sample - loss: 0.0699 - accuracy: 0.9823 - val_loss: 0.6610 - val_accuracy: 0.8317\n",
      "Epoch 7/20\n",
      "18676/18676 [==============================] - 20s 1ms/sample - loss: 0.0604 - accuracy: 0.9847 - val_loss: 0.6942 - val_accuracy: 0.8287\n",
      "Epoch 8/20\n",
      "18676/18676 [==============================] - 20s 1ms/sample - loss: 0.0445 - accuracy: 0.9896 - val_loss: 0.7037 - val_accuracy: 0.8330\n",
      "Epoch 9/20\n",
      "18676/18676 [==============================] - 20s 1ms/sample - loss: 0.0461 - accuracy: 0.9880 - val_loss: 0.7207 - val_accuracy: 0.8216\n",
      "Epoch 10/20\n",
      "18676/18676 [==============================] - 20s 1ms/sample - loss: 0.0498 - accuracy: 0.9871 - val_loss: 0.7204 - val_accuracy: 0.8261\n",
      "Epoch 11/20\n",
      "18676/18676 [==============================] - 22s 1ms/sample - loss: 0.0412 - accuracy: 0.9900 - val_loss: 0.7552 - val_accuracy: 0.8291\n",
      "Epoch 12/20\n",
      "18676/18676 [==============================] - 21s 1ms/sample - loss: 0.0397 - accuracy: 0.9900 - val_loss: 0.7887 - val_accuracy: 0.8231\n",
      "Epoch 13/20\n",
      "18676/18676 [==============================] - 21s 1ms/sample - loss: 0.0247 - accuracy: 0.9943 - val_loss: 0.7457 - val_accuracy: 0.8336\n",
      "Epoch 14/20\n",
      "18676/18676 [==============================] - 20s 1ms/sample - loss: 0.0232 - accuracy: 0.9946 - val_loss: 0.8031 - val_accuracy: 0.8315\n",
      "Epoch 15/20\n",
      "18676/18676 [==============================] - 21s 1ms/sample - loss: 0.0231 - accuracy: 0.9951 - val_loss: 0.7817 - val_accuracy: 0.8268\n",
      "Epoch 16/20\n",
      "18676/18676 [==============================] - 21s 1ms/sample - loss: 0.0365 - accuracy: 0.9902 - val_loss: 0.7543 - val_accuracy: 0.8315\n",
      "Epoch 17/20\n",
      "18676/18676 [==============================] - 21s 1ms/sample - loss: 0.0207 - accuracy: 0.9954 - val_loss: 0.8340 - val_accuracy: 0.8306\n",
      "Epoch 18/20\n",
      "18676/18676 [==============================] - 21s 1ms/sample - loss: 0.0140 - accuracy: 0.9974 - val_loss: 0.8655 - val_accuracy: 0.8289\n",
      "Epoch 19/20\n",
      "18676/18676 [==============================] - 22s 1ms/sample - loss: 0.0161 - accuracy: 0.9960 - val_loss: 0.8800 - val_accuracy: 0.8240\n",
      "Epoch 20/20\n",
      "18676/18676 [==============================] - 20s 1ms/sample - loss: 0.0225 - accuracy: 0.9944 - val_loss: 0.8784 - val_accuracy: 0.8141\n"
     ]
    }
   ],
   "source": [
    "model_lstm4 = tf.keras.Sequential([layers.Masking(), tf.keras.layers.LSTM(128), layers.Dense(GESTURE_TYPES), layers.Activation('softmax')])\n",
    "model_lstm4.compile(loss=tf.losses.SparseCategoricalCrossentropy(), optimizer=tf.optimizers.Adam(), metrics=['accuracy'])\n",
    "history = model_lstm4.fit(X, labels, epochs=20, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing dense layer depth may improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18676 samples, validate on 4670 samples\n",
      "Epoch 1/20\n",
      "18676/18676 [==============================] - 58s 3ms/sample - loss: 0.4245 - accuracy: 0.8673 - val_loss: 0.6317 - val_accuracy: 0.8090\n",
      "Epoch 2/20\n",
      "18676/18676 [==============================] - 41s 2ms/sample - loss: 0.2202 - accuracy: 0.9286 - val_loss: 0.5720 - val_accuracy: 0.8313\n",
      "Epoch 3/20\n",
      "18676/18676 [==============================] - 39s 2ms/sample - loss: 0.1491 - accuracy: 0.9512 - val_loss: 0.7063 - val_accuracy: 0.8139\n",
      "Epoch 4/20\n",
      "18676/18676 [==============================] - 40s 2ms/sample - loss: 0.1199 - accuracy: 0.9620 - val_loss: 0.7088 - val_accuracy: 0.8180\n",
      "Epoch 5/20\n",
      "18676/18676 [==============================] - 37s 2ms/sample - loss: 0.0935 - accuracy: 0.9701 - val_loss: 0.7491 - val_accuracy: 0.8278\n",
      "Epoch 6/20\n",
      "18676/18676 [==============================] - 42s 2ms/sample - loss: 0.0701 - accuracy: 0.9780 - val_loss: 0.7608 - val_accuracy: 0.8201\n",
      "Epoch 7/20\n",
      "18676/18676 [==============================] - 48s 3ms/sample - loss: 0.0727 - accuracy: 0.9771 - val_loss: 0.7755 - val_accuracy: 0.8255\n",
      "Epoch 8/20\n",
      "18676/18676 [==============================] - 46s 2ms/sample - loss: 0.0492 - accuracy: 0.9849 - val_loss: 0.9048 - val_accuracy: 0.8223\n",
      "Epoch 9/20\n",
      "18676/18676 [==============================] - 45s 2ms/sample - loss: 0.0513 - accuracy: 0.9842 - val_loss: 0.9148 - val_accuracy: 0.8133\n",
      "Epoch 10/20\n",
      "18676/18676 [==============================] - 45s 2ms/sample - loss: 0.0421 - accuracy: 0.9870 - val_loss: 0.9839 - val_accuracy: 0.8116\n",
      "Epoch 11/20\n",
      "18676/18676 [==============================] - 45s 2ms/sample - loss: 0.0319 - accuracy: 0.9907 - val_loss: 1.0223 - val_accuracy: 0.8122\n",
      "Epoch 12/20\n",
      "18676/18676 [==============================] - 46s 2ms/sample - loss: 0.0471 - accuracy: 0.9855 - val_loss: 0.9666 - val_accuracy: 0.8096\n",
      "Epoch 13/20\n",
      "18676/18676 [==============================] - 43s 2ms/sample - loss: 0.0354 - accuracy: 0.9905 - val_loss: 0.9847 - val_accuracy: 0.8195\n",
      "Epoch 14/20\n",
      "18676/18676 [==============================] - 43s 2ms/sample - loss: 0.0305 - accuracy: 0.9907 - val_loss: 1.0469 - val_accuracy: 0.8075\n",
      "Epoch 15/20\n",
      "18676/18676 [==============================] - 44s 2ms/sample - loss: 0.0401 - accuracy: 0.9873 - val_loss: 0.9604 - val_accuracy: 0.8193\n",
      "Epoch 16/20\n",
      "18676/18676 [==============================] - 43s 2ms/sample - loss: 0.0194 - accuracy: 0.9950 - val_loss: 1.0792 - val_accuracy: 0.8158\n",
      "Epoch 17/20\n",
      "18676/18676 [==============================] - 43s 2ms/sample - loss: 0.0266 - accuracy: 0.9928 - val_loss: 1.0736 - val_accuracy: 0.8101\n",
      "Epoch 18/20\n",
      "18676/18676 [==============================] - 45s 2ms/sample - loss: 0.0265 - accuracy: 0.9924 - val_loss: 1.1231 - val_accuracy: 0.8139\n",
      "Epoch 19/20\n",
      "18676/18676 [==============================] - 45s 2ms/sample - loss: 0.0324 - accuracy: 0.9907 - val_loss: 1.0969 - val_accuracy: 0.8120\n",
      "Epoch 20/20\n",
      "18676/18676 [==============================] - 43s 2ms/sample - loss: 0.0356 - accuracy: 0.9893 - val_loss: 0.9997 - val_accuracy: 0.8069\n"
     ]
    }
   ],
   "source": [
    "model_lstm5 = tf.keras.Sequential([layers.Masking(), tf.keras.layers.LSTM(128,return_sequences=True),tf.keras.layers.LSTM(128), layers.Dense(GESTURE_TYPES), layers.Activation('softmax')])\n",
    "model_lstm5.compile(loss=tf.losses.SparseCategoricalCrossentropy(), optimizer=tf.optimizers.Adam(), metrics=['accuracy'])\n",
    "history = model_lstm5.fit(X, labels, epochs=20, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding another layer of LSTM does not improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional + Recurrent\n",
    "[Núñez et al. - Convolutional Neural Networks and Long Short-Term Memory for skeleton-based human activity and hand gesture recognition](https://www.sciencedirect.com/science/article/pii/S0031320317304405)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18676 samples, validate on 4670 samples\n",
      "Epoch 1/20\n",
      "18676/18676 [==============================] - 13s 688us/sample - loss: 0.6499 - accuracy: 0.7915 - val_loss: 0.6435 - val_accuracy: 0.7953\n",
      "Epoch 2/20\n",
      "18676/18676 [==============================] - 9s 486us/sample - loss: 0.3910 - accuracy: 0.8702 - val_loss: 0.6158 - val_accuracy: 0.8045\n",
      "Epoch 3/20\n",
      "18676/18676 [==============================] - 9s 497us/sample - loss: 0.3348 - accuracy: 0.8911 - val_loss: 0.6311 - val_accuracy: 0.8026\n",
      "Epoch 4/20\n",
      "18676/18676 [==============================] - 10s 560us/sample - loss: 0.3003 - accuracy: 0.8982 - val_loss: 0.6754 - val_accuracy: 0.7949\n",
      "Epoch 5/20\n",
      "18676/18676 [==============================] - 10s 525us/sample - loss: 0.2730 - accuracy: 0.9090 - val_loss: 0.6451 - val_accuracy: 0.8122\n",
      "Epoch 6/20\n",
      "18676/18676 [==============================] - 10s 518us/sample - loss: 0.2506 - accuracy: 0.9143 - val_loss: 0.7023 - val_accuracy: 0.8009\n",
      "Epoch 7/20\n",
      "18676/18676 [==============================] - 10s 522us/sample - loss: 0.2334 - accuracy: 0.9203 - val_loss: 0.6789 - val_accuracy: 0.8092\n",
      "Epoch 8/20\n",
      "18676/18676 [==============================] - 10s 525us/sample - loss: 0.2165 - accuracy: 0.9249 - val_loss: 0.6615 - val_accuracy: 0.8171\n",
      "Epoch 9/20\n",
      "18676/18676 [==============================] - 10s 512us/sample - loss: 0.2020 - accuracy: 0.9299 - val_loss: 0.7487 - val_accuracy: 0.8113\n",
      "Epoch 10/20\n",
      "18676/18676 [==============================] - 10s 523us/sample - loss: 0.1903 - accuracy: 0.9331 - val_loss: 0.7953 - val_accuracy: 0.8113\n",
      "Epoch 11/20\n",
      "18676/18676 [==============================] - 10s 558us/sample - loss: 0.1829 - accuracy: 0.9359 - val_loss: 0.7167 - val_accuracy: 0.8158\n",
      "Epoch 12/20\n",
      "18676/18676 [==============================] - 10s 532us/sample - loss: 0.1720 - accuracy: 0.9400 - val_loss: 0.8075 - val_accuracy: 0.8096\n",
      "Epoch 13/20\n",
      "18676/18676 [==============================] - 9s 507us/sample - loss: 0.1683 - accuracy: 0.9394 - val_loss: 0.7725 - val_accuracy: 0.8111\n",
      "Epoch 14/20\n",
      "18676/18676 [==============================] - 9s 500us/sample - loss: 0.1576 - accuracy: 0.9441 - val_loss: 0.7851 - val_accuracy: 0.8141\n",
      "Epoch 15/20\n",
      "18676/18676 [==============================] - 9s 500us/sample - loss: 0.1510 - accuracy: 0.9474 - val_loss: 0.8138 - val_accuracy: 0.8128\n",
      "Epoch 16/20\n",
      "18676/18676 [==============================] - 9s 499us/sample - loss: 0.1513 - accuracy: 0.9458 - val_loss: 0.7904 - val_accuracy: 0.8141\n",
      "Epoch 17/20\n",
      "18676/18676 [==============================] - 9s 504us/sample - loss: 0.1421 - accuracy: 0.9503 - val_loss: 0.8402 - val_accuracy: 0.8079\n",
      "Epoch 18/20\n",
      "18676/18676 [==============================] - 10s 514us/sample - loss: 0.1358 - accuracy: 0.9513 - val_loss: 0.9069 - val_accuracy: 0.8056\n",
      "Epoch 19/20\n",
      "18676/18676 [==============================] - 10s 510us/sample - loss: 0.1369 - accuracy: 0.9505 - val_loss: 0.8677 - val_accuracy: 0.8090\n",
      "Epoch 20/20\n",
      "18676/18676 [==============================] - 9s 498us/sample - loss: 0.1255 - accuracy: 0.9550 - val_loss: 0.9006 - val_accuracy: 0.8034\n"
     ]
    }
   ],
   "source": [
    "model_cnnlstm2 = tf.keras.Sequential([layers.Masking(), layers.Conv1D(20,3,activation='relu'),layers.Conv1D(20,3,activation='relu'), layers.LSTM(128, return_sequences=True), layers.Dense(GESTURE_TYPES, activation='softmax')])\n",
    "model_cnnlstm2.compile(loss=tf.losses.SparseCategoricalCrossentropy(), optimizer=tf.optimizers.Adam(), metrics=['accuracy'])\n",
    "history = model_cnnlstm2.fit(X, labels, epochs=20, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
